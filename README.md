## 自动文本摘要
- 本项目是单文本自动摘要
- 自动文本摘要分为 `抽取式摘要` 和 `生成式摘要`。     
- 抽取式： `TextRank`
- 生成式： `Seq2Seq + attention`

### PageRank 算法
`TextRank` 要从 `PageRank` 说起。  
许多实际的数据都是以 `图` 的形式存在，不如互联网、社交网络； `PageRank` 算法是图的 `链接分析(link analysis)` 为代表的算法，是图数据上的 `无监督学习方法`。  
`PageRank` 应用在 **有向图**上，最初是作为互联网网页重要程度的计算方法，后来被应用到 `文本摘要` 等多个领域。  
`PageRank` 算法的基本想法是在 `有向图` 上定义一个 `随机游走模型`，即 `一阶马尔科夫链`，在一定条件下，极限情况访问每一个结点的概率收敛到 `平稳分布`，这时各个结点的的平稳概率值就是其 `PageRank` 值，该值可以表示网页的重要程度。    

目标是 **求得访问各个结点的概率分布**，该概率分布就是 **一阶马尔科夫链** 的 **平稳分布**。   

具体的过程是这样的：    
先随机给点一个关于各个结点的初始概率分布，假设是 $n$ 个结点，初始概率分布为 $R_0=[1/n,...,1/n]^T$，维度是 $R^{n \times 1}$。一阶马尔科夫链在各个时刻都有一个固定的转移矩阵 $M$，维度为$R^{n \times n}$，在 $t=1、2、3,...t,...$ 时刻的概率分布为
$$
M R_0, M^2 R_0 \quad, M^3 R_0, \quad, ..., \quad, M^t R_0, \quad, ...
$$
转移矩阵 $M$ 的一个例子如下
$$
M = 
\left[
  \begin{matrix}
 & A & B & C \\
 A &  1/3 & 1/2 & 0 \\
 B &  1/3 & 0 &  1/2 \\
 C &  1/3 & 1/2 & 1/2
  \end{matrix} 
\right]     \tag{1}
$$
每一列代表该状态转移到其他状态的概率，比如第一列，意思是 $A$ 转移到 $A、B、C$ 的概率分别是$\{1/3, 1/3，1/3\}$，为什么都是 $1/3$ 呢？这是因为有向图中，结点 $A$ 指向了 $A、B、C$，这里我们假定是 **一个结点到其连出的所有结点的转移概率相等**，注意是连出的结点，所以是 $1/3$。  
每一行代表其他状态转到该状态的概率，比如第一行，意思是 $A、B、C$ 转移到 $A$ 的概率分别是 $\{1/3, 1/2，0\}$，如果初始的概率分布是 $R_0=[1/3, 1/3, 1/3]$，初始概率分布的意思是 **0 时刻是状态 $A,B,C$ 的概率分别是 1/3, 1/3, 1/3**。那么下一个时刻是 $A$ 的概率，自然是当前各状态的概率 `乘以` 各自转移到 $A$ 的概率，然后再求和，也就是 $M$ 第一行的行向量与 $R_0$ 的内积：
$$
p_{t=1}(A) = [1/3, 1/2, 0][1/3, 1/3, 1/3]^T = 5/18  
$$
同理，下一个时刻是 $B、C$ 的概率和 $A$ 一样求法。写成矩阵的形式就是：
$$
R_1 = M R_0
$$

而稳定分布是下面极限存在：
$$
\lim_{t \rightarrow \infty} M^t R_0 = R   \tag{2}
$$
$R$ 就是平稳分布，满足
$$
MR=R
$$

$(2)$ 极限存在的条件是 $n$ 个结点构成的有向图是 **强连通且非周期的有向图**，强连通是指 从任意一个结点出发，都可以沿着箭头到达任意的结点。  但是明显，对于网页来说，或者很多的实际场景来说，有些结点(有些网页内部没有其他网页的链接)没有 **出度**，所以不是强连通的，所以就不存在平稳分布，那么也就没法得到 `pagerank` 值。  

如果想对非强连通图找到平稳分布，需要对上面的公式进行修正，就是在上面的基础上导入 **平滑项**。 
具体是这样的，假设当前在某个网页上，以概率 $d$ 按照转移矩阵的进行跳转，以概率 $(1-d)$ 按照等概论 $1/n$ 跳转到任意其他网页。  
两者的线性组合构成一个新的转移矩阵，该转移矩阵定义的一阶马尔科夫链的平稳分布 $R$ 是存在的，满足：
$$
R = dMR + \frac{1-d}{n} \bold 1
$$

###  TextRank 算法
`TextRank` 可以说是 `PageRank` 在 `文本摘要` 中的应用，具体：
- 用句子代替网页;
- 任意两个网页的相似度等价于网页的转化概率;
- 相似性的得分存储在一个方形矩阵中，类似于 `pagerank` 的转移概率矩阵 $M$；  
具体的实施可以参见 [使用 TextRank 算法为文本生成关键字和摘要](https://www.letianbiji.com/machine-learning/text-rank.html)


### Seq2Seq + Attention

1. `encoder` 使用单层双向 `GRU`, 最后一层的输出不是拼接，而是相加
2. `decoder` 使用双层单项 `GRU`，第一个字符 `<bos>` 输入的 `hidden` 使用 `encoder` 端的输出初始化的
3. 为了架构清晰，`encoder` 和 `decoder` 的 `hidden_size` 取相同值，并且取 `hidden_siz = embed_size`

